{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robin_stocks as r\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#Log in to Robinhood\n",
    "robinhood_login = r.login('lkleinbrodt@gmail.com', 'RIPmamba24!robinhood')\n",
    "\n",
    "#Get Alpaca APIs\n",
    "with open('api_keys.txt') as api_file:\n",
    "    api_keys = api_file.read().replace('\\n', '').split(',')\n",
    "    alpaca_api = {a.split('=')[0]: a.split('=')[1] for a in api_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/historical_data.csv')\n",
    "df = df.dropna()\n",
    "ori_df = df.copy()\n",
    "dates = pd.to_datetime(df.pop('begins_at'))\n",
    "#keep only the close price columns\n",
    "df = df[[col for col in df.columns if 'close_price' in col]]\n",
    "#df = df[df.columns[np.argsort(df.sum()).isin(range(25))]]\n",
    "#keep only the funds with more than 10 unique value in the first year\n",
    "#unique_counts = df.head(365).nunique()\n",
    "#df = df[unique_counts.keys()[unique_counts>10].values]\n",
    "df = df[['AAPL_close_price', 'AAXN_close_price']]\n",
    "df = df.diff()\n",
    "df = df[1:]\n",
    "ori_df = ori_df[1:]\n",
    "dates = dates[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end= np.min(dates) + relativedelta(months = 44)\n",
    "val_end = train_end + relativedelta(months = 10) #leaves 6 months to backtest\n",
    "\n",
    "train_df = df[dates <= train_end]\n",
    "val_df = df[(dates > train_end) & (dates <= val_end)]\n",
    "test_df = df[dates > val_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, \n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                label_columns=None):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        #Index the labels (and all columns)\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i,name in enumerate(train_df.columns)}\n",
    "        \n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis = -1)\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        return inputs, labels\n",
    "    \n",
    "    def plot(self, plot_col, model=None, max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(3, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "            \n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "            \n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                          marker='X', edgecolors='k', label='Predictions',\n",
    "                          c='#ff7f0e', s=64)\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "        plt.xlabel('Time [d]')\n",
    "    \n",
    "    def make_dataset(self, data, shuffle = True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=32\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "@property\n",
    "def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "        result = next(iter(self.train))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "    return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "def compile_and_fit(model, window, epochs = MAX_EPOCHS, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=patience,\n",
    "                                                      mode='min')\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(), \n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    history = model.fit(window.train, epochs = epochs,\n",
    "                        validation_data=window.val,\n",
    "                        #callbacks=[early_stopping]\n",
    "                       )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 1.3626 - mean_absolute_error: 0.6704 - val_loss: 7.9251 - val_mean_absolute_error: 1.9069\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 1.3615 - mean_absolute_error: 0.6698 - val_loss: 7.9239 - val_mean_absolute_error: 1.9065\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 1.3607 - mean_absolute_error: 0.6694 - val_loss: 7.9229 - val_mean_absolute_error: 1.9062\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 1.3599 - mean_absolute_error: 0.6690 - val_loss: 7.9218 - val_mean_absolute_error: 1.9059\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3594 - mean_absolute_error: 0.6688 - val_loss: 7.9209 - val_mean_absolute_error: 1.9056\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 1.3587 - mean_absolute_error: 0.6686 - val_loss: 7.9200 - val_mean_absolute_error: 1.9053\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 1.3584 - mean_absolute_error: 0.6686 - val_loss: 7.9191 - val_mean_absolute_error: 1.9051\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 1.3579 - mean_absolute_error: 0.6686 - val_loss: 7.9181 - val_mean_absolute_error: 1.9049\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 1.3577 - mean_absolute_error: 0.6687 - val_loss: 7.9173 - val_mean_absolute_error: 1.9048\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 1.3575 - mean_absolute_error: 0.6689 - val_loss: 7.9169 - val_mean_absolute_error: 1.9047\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 1.3571 - mean_absolute_error: 0.6689 - val_loss: 7.9158 - val_mean_absolute_error: 1.9045\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 1.3569 - mean_absolute_error: 0.6691 - val_loss: 7.9149 - val_mean_absolute_error: 1.9044\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 1.3566 - mean_absolute_error: 0.6691 - val_loss: 7.9142 - val_mean_absolute_error: 1.9043\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 1.3562 - mean_absolute_error: 0.6691 - val_loss: 7.9136 - val_mean_absolute_error: 1.9042\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3559 - mean_absolute_error: 0.6691 - val_loss: 7.9122 - val_mean_absolute_error: 1.9040\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3553 - mean_absolute_error: 0.6691 - val_loss: 7.9102 - val_mean_absolute_error: 1.9036\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 1.3548 - mean_absolute_error: 0.6692 - val_loss: 7.9080 - val_mean_absolute_error: 1.9032\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 1.3539 - mean_absolute_error: 0.6691 - val_loss: 7.9048 - val_mean_absolute_error: 1.9025\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 1.3527 - mean_absolute_error: 0.6690 - val_loss: 7.9004 - val_mean_absolute_error: 1.9015\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3509 - mean_absolute_error: 0.6688 - val_loss: 7.8951 - val_mean_absolute_error: 1.9003\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 1.3485 - mean_absolute_error: 0.6683 - val_loss: 7.8889 - val_mean_absolute_error: 1.8988\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3455 - mean_absolute_error: 0.6676 - val_loss: 7.8844 - val_mean_absolute_error: 1.8975\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3425 - mean_absolute_error: 0.6672 - val_loss: 7.8795 - val_mean_absolute_error: 1.8961\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3398 - mean_absolute_error: 0.6665 - val_loss: 7.8746 - val_mean_absolute_error: 1.8949\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 1.3368 - mean_absolute_error: 0.6661 - val_loss: 7.8692 - val_mean_absolute_error: 1.8936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb31703ce80>"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONV_WIDTH = 3\n",
    "OUT_STEPS = 7\n",
    "num_features = df.shape[1]\n",
    "\n",
    "multi_window = WindowGenerator(input_width = 60, label_width = OUT_STEPS, shift = OUT_STEPS, label_columns=['AAXN_close_price'])\n",
    "\n",
    "one_shot_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(4, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    #tf.keras.layers.Reshape([OUT_STEPS, 1])\n",
    "])\n",
    "\n",
    "multi_dense_model = tf.keras.Sequential([\n",
    "    # Take the last time step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, 1])\n",
    "])\n",
    "\n",
    "compile_and_fit(one_shot_lstm_model, multi_window, epochs = 25)\n",
    "#compile_and_fit(multi_dense_model, multi_window, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.3916 - mean_absolute_error: 0.4125 - val_loss: 4.1666 - val_mean_absolute_error: 1.3086\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.3872 - mean_absolute_error: 0.4097 - val_loss: 4.1627 - val_mean_absolute_error: 1.3044\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.3855 - mean_absolute_error: 0.4098 - val_loss: 4.1649 - val_mean_absolute_error: 1.3006\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.3828 - mean_absolute_error: 0.4104 - val_loss: 4.1898 - val_mean_absolute_error: 1.3069\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.3799 - mean_absolute_error: 0.4087 - val_loss: 4.2284 - val_mean_absolute_error: 1.3205\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.3790 - mean_absolute_error: 0.4085 - val_loss: 4.2459 - val_mean_absolute_error: 1.3266\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.3777 - mean_absolute_error: 0.4073 - val_loss: 4.2412 - val_mean_absolute_error: 1.3246\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.3757 - mean_absolute_error: 0.4067 - val_loss: 4.2382 - val_mean_absolute_error: 1.3233\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.3729 - mean_absolute_error: 0.4049 - val_loss: 4.2781 - val_mean_absolute_error: 1.3333\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.3711 - mean_absolute_error: 0.4045 - val_loss: 4.2721 - val_mean_absolute_error: 1.3329\n"
     ]
    }
   ],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "  def __init__(self, units, out_steps):\n",
    "    super().__init__()\n",
    "    self.out_steps = out_steps\n",
    "    self.units = units\n",
    "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(num_features)\n",
    "    \n",
    "def warmup(self, inputs):\n",
    "  # inputs.shape => (batch, time, features)\n",
    "  # x.shape => (batch, lstm_units)\n",
    "  x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "  # predictions.shape => (batch, features)\n",
    "  prediction = self.dense(x)\n",
    "  return prediction, state\n",
    "\n",
    "FeedBack.warmup = warmup\n",
    "\n",
    "def call(self, inputs, training=None):\n",
    "  # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "  predictions = []\n",
    "  # Initialize the lstm state\n",
    "  prediction, state = self.warmup(inputs)\n",
    "\n",
    "  # Insert the first prediction\n",
    "  predictions.append(prediction)\n",
    "\n",
    "  # Run the rest of the prediction steps\n",
    "  for n in range(1, self.out_steps):\n",
    "    # Use the last prediction as input.\n",
    "    x = prediction\n",
    "    # Execute one lstm step.\n",
    "    x, state = self.lstm_cell(x, states=state,\n",
    "                              training=training)\n",
    "    # Convert the lstm output to a prediction.\n",
    "    prediction = self.dense(x)\n",
    "    # Add the prediction to the output\n",
    "    predictions.append(prediction)\n",
    "\n",
    "  # predictions.shape => (time, batch, features)\n",
    "  predictions = tf.stack(predictions)\n",
    "  # predictions.shape => (batch, time, features)\n",
    "  predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "  return predictions\n",
    "\n",
    "FeedBack.call = call\n",
    "feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)\n",
    "history = compile_and_fit(feedback_model, multi_window, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_inputs = multi_window.input_width\n",
    "#n_features = train_df.shape[1]\n",
    "#batch = np.array(test_df[-n_inputs:]).reshape((1, n_inputs, n_features))\n",
    "#one_shot_lstm_model.predict(batch)[0,:,:][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = one_shot_lstm_model.predict(multi_window.make_dataset(df, shuffle = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day_deltas = [0 for i in range(len(df) - len(all_predictions)-1)] + [np.max(np.cumsum(pred)) for pred in all_predictions] + [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_cash = 1000\n",
    "eval_df = ori_df[['AAPL_close_price']].copy()\n",
    "eval_df['SevenDayHigh'] = seven_day_deltas\n",
    "cash_reserves = starting_cash\n",
    "eval_df['Cash'] = 0\n",
    "eval_df['Cost'] = 0\n",
    "eval_df['Revenue'] = 0\n",
    "eval_df['Shares'] = 0\n",
    "eval_df['PortfolioValue'] = 0\n",
    "eval_df = eval_df[dates > val_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/landon/opt/anaconda3/envs/auctioneer/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(eval_df)-1):\n",
    "    max_delta = eval_df['SevenDayHigh'].iloc[i]\n",
    "    eval_df['Cash'].iloc[i] = cash_reserves\n",
    "    if max_delta > 0:\n",
    "        if eval_df['AAPL_close_price'].iloc[i] < cash_reserves:\n",
    "            eval_df['Cost'].iloc[i] = eval_df['AAPL_close_price'].iloc[i]\n",
    "            eval_df['Shares'].iloc[i+1] = eval_df['Shares'].iloc[i] + 1\n",
    "            cash_reserves -= eval_df['AAPL_close_price'].iloc[i]\n",
    "        else:\n",
    "            eval_df['Shares'].iloc[i+1] = eval_df['Shares'].iloc[i]\n",
    "    elif eval_df['Shares'].iloc[i] > 0:\n",
    "        eval_df['Revenue'].iloc[i] = eval_df['AAPL_close_price'].iloc[i]\n",
    "        eval_df['Shares'].iloc[i+1] = eval_df['Shares'].iloc[i] - 1\n",
    "        cash_reserves += eval_df['AAPL_close_price'].iloc[i]\n",
    "        \n",
    "    eval_df['PortfolioValue'].iloc[i] = (eval_df['AAPL_close_price'].iloc[i] * eval_df['Shares'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cost = np.sum(eval_df['Cost'])\n",
    "final_revenue = np.sum(eval_df['Revenue'])\n",
    "final_assets = eval_df.iloc[-2]['PortfolioValue']\n",
    "final_cash = cash_reserves\n",
    "final_return = final_assets + final_cash #- final_cost \n",
    "\n",
    "long_term_return = (eval_df['AAPL_close_price'].iloc[-2] / eval_df['AAPL_close_price'].iloc[0])\n",
    "\n",
    "profit_score = ((final_return) / starting_cash)  / long_term_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4039775"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_return / starting_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447517320734341"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auctioneer",
   "language": "python",
   "name": "auctioneer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
